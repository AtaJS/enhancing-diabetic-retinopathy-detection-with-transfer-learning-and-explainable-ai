{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7NF31mO8fWV-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735110845942,"user_tz":-60,"elapsed":31957,"user":{"displayName":"Amir sard","userId":"05229660649413556744"}},"outputId":"f90bee65-949a-41c9-9272-366c014f052d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import copy\n","import os\n","import random\n","import sys\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from PIL import Image\n","from sklearn.metrics import cohen_kappa_score, precision_score, recall_score, accuracy_score\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models, transforms\n","from torchvision.transforms.functional import to_pil_image\n","from tqdm import tqdm\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/DeepLearning_Final_Project_2024')\n","print(os.listdir('DeepDRiD'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRapdiW2mPfO","executionInfo":{"status":"ok","timestamp":1735110850322,"user_tz":-60,"elapsed":1558,"user":{"displayName":"Amir sard","userId":"05229660649413556744"}},"outputId":"4570440a-41fa-498c-9f2b-cd660cde8cf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['sample_submission.csv', 'test.csv', 'train.csv', 'val.csv', 'test', 'train', 'val']\n"]}]},{"cell_type":"code","source":["\n","# Hyper Parameters\n","batch_size = 24\n","num_classes = 5  # 5 DR levels\n","learning_rate = 0.0001\n","num_epochs = 20\n","\n","\n","class RetinopathyDataset(Dataset):\n","    def __init__(self, ann_file, image_dir, transform=None, mode='single', test=False):\n","        self.ann_file = ann_file\n","        self.image_dir = image_dir\n","        self.transform = transform\n","\n","        self.test = test\n","        self.mode = mode\n","\n","        if self.mode == 'single':\n","            self.data = self.load_data()\n","        else:\n","            self.data = self.load_data_dual()\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        if self.mode == 'single':\n","            return self.get_item(index)\n","        else:\n","            return self.get_item_dual(index)\n","\n","    # 1. single image\n","    def load_data(self):\n","        df = pd.read_csv(self.ann_file)\n","\n","        data = []\n","        for _, row in df.iterrows():\n","            file_info = dict()\n","            file_info['img_path'] = os.path.join(self.image_dir, row['img_path'])\n","            if not self.test:\n","                file_info['dr_level'] = int(row['patient_DR_Level'])\n","            data.append(file_info)\n","        return data\n","\n","    def get_item(self, index):\n","        data = self.data[index]\n","        img = Image.open(data['img_path']).convert('RGB')\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        if not self.test:\n","            label = torch.tensor(data['dr_level'], dtype=torch.int64)\n","            return img, label\n","        else:\n","            return img\n","\n","    # 2. dual image\n","    def load_data_dual(self):\n","        df = pd.read_csv(self.ann_file)\n","\n","        df['prefix'] = df['image_id'].str.split('_').str[0]  # The patient id of each image\n","        df['suffix'] = df['image_id'].str.split('_').str[1].str[0]  # The left or right eye\n","        grouped = df.groupby(['prefix', 'suffix'])\n","\n","        data = []\n","        for (prefix, suffix), group in grouped:\n","            file_info = dict()\n","            file_info['img_path1'] = os.path.join(self.image_dir, group.iloc[0]['img_path'])\n","            file_info['img_path2'] = os.path.join(self.image_dir, group.iloc[1]['img_path'])\n","            if not self.test:\n","                file_info['dr_level'] = int(group.iloc[0]['patient_DR_Level'])\n","            data.append(file_info)\n","        return data\n","\n","    def get_item_dual(self, index):\n","        data = self.data[index]\n","        img1 = Image.open(data['img_path1']).convert('RGB')\n","        img2 = Image.open(data['img_path2']).convert('RGB')\n","\n","        if self.transform:\n","            img1 = self.transform(img1)\n","            img2 = self.transform(img2)\n","\n","        if not self.test:\n","            label = torch.tensor(data['dr_level'], dtype=torch.int64)\n","            return [img1, img2], label\n","        else:\n","            return [img1, img2]\n","\n","\n","class CutOut(object):\n","    def __init__(self, mask_size, p=0.5):\n","        self.mask_size = mask_size\n","        self.p = p\n","\n","    def __call__(self, img):\n","        if np.random.rand() > self.p:\n","            return img\n","\n","        # Ensure the image is a tensor\n","        if not isinstance(img, torch.Tensor):\n","            raise TypeError('Input image must be a torch.Tensor')\n","\n","        # Get height and width of the image\n","        h, w = img.shape[1], img.shape[2]\n","        mask_size_half = self.mask_size // 2\n","        offset = 1 if self.mask_size % 2 == 0 else 0\n","\n","        cx = np.random.randint(mask_size_half, w + offset - mask_size_half)\n","        cy = np.random.randint(mask_size_half, h + offset - mask_size_half)\n","\n","        xmin, xmax = cx - mask_size_half, cx + mask_size_half + offset\n","        ymin, ymax = cy - mask_size_half, cy + mask_size_half + offset\n","        xmin, xmax = max(0, xmin), min(w, xmax)\n","        ymin, ymax = max(0, ymin), min(h, ymax)\n","\n","        img[:, ymin:ymax, xmin:xmax] = 0\n","        return img\n","\n","\n","class SLORandomPad:\n","    def __init__(self, size):\n","        self.size = size\n","\n","    def __call__(self, img):\n","        pad_width = max(0, self.size[0] - img.width)\n","        pad_height = max(0, self.size[1] - img.height)\n","        pad_left = random.randint(0, pad_width)\n","        pad_top = random.randint(0, pad_height)\n","        pad_right = pad_width - pad_left\n","        pad_bottom = pad_height - pad_top\n","        return transforms.functional.pad(img, (pad_left, pad_top, pad_right, pad_bottom))\n","\n","\n","class FundRandomRotate:\n","    def __init__(self, prob, degree):\n","        self.prob = prob\n","        self.degree = degree\n","\n","    def __call__(self, img):\n","        if random.random() < self.prob:\n","            angle = random.uniform(-self.degree, self.degree)\n","            return transforms.functional.rotate(img, angle)\n","        return img\n","\n","\n","transform_train = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.RandomCrop((210, 210)),\n","    SLORandomPad((224, 224)),\n","    FundRandomRotate(prob=0.5, degree=30),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomVerticalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=(0.1, 0.9)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","def train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=25,\n","                checkpoint_path='model.pth'):\n","    best_model = model.state_dict()\n","    best_epoch = None\n","    best_val_kappa = -1.0  # Initialize the best kappa score\n","\n","    for epoch in range(1, num_epochs + 1):\n","        print(f'\\nEpoch {epoch}/{num_epochs}')\n","        running_loss = []\n","        all_preds = []\n","        all_labels = []\n","\n","        model.train()\n","\n","        with tqdm(total=len(train_loader), desc=f'Training', unit=' batch', file=sys.stdout) as pbar:\n","            for images, labels in train_loader:\n","                if not isinstance(images, list):\n","                    images = images.to(device)  # single image case\n","                else:\n","                    images = [x.to(device) for x in images]  # dual images case\n","\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                outputs = model(images)\n","                loss = criterion(outputs, labels.long())\n","\n","                loss.backward()\n","                optimizer.step()\n","\n","                preds = torch.argmax(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","                running_loss.append(loss.item())\n","\n","                pbar.set_postfix({'lr': f'{optimizer.param_groups[0][\"lr\"]:.1e}', 'Loss': f'{loss.item():.4f}'})\n","                pbar.update(1)\n","\n","        lr_scheduler.step()\n","\n","        epoch_loss = sum(running_loss) / len(running_loss)\n","\n","        train_metrics = compute_metrics(all_preds, all_labels, per_class=True)\n","        kappa, accuracy, precision, recall = train_metrics[:4]\n","\n","        print(f'[Train] Kappa: {kappa:.4f} Accuracy: {accuracy:.4f} '\n","              f'Precision: {precision:.4f} Recall: {recall:.4f} Loss: {epoch_loss:.4f}')\n","\n","        if len(train_metrics) > 4:\n","            precision_per_class, recall_per_class = train_metrics[4:]\n","            for i, (precision, recall) in enumerate(zip(precision_per_class, recall_per_class)):\n","                print(f'[Train] Class {i}: Precision: {precision:.4f}, Recall: {recall:.4f}')\n","\n","        # Evaluation on the validation set at the end of each epoch\n","        val_metrics = evaluate_model(model, val_loader, device)\n","        val_kappa, val_accuracy, val_precision, val_recall = val_metrics[:4]\n","        print(f'[Val] Kappa: {val_kappa:.4f} Accuracy: {val_accuracy:.4f} '\n","              f'Precision: {val_precision:.4f} Recall: {val_recall:.4f}')\n","\n","        if val_kappa > best_val_kappa:\n","            best_val_kappa = val_kappa\n","            best_epoch = epoch\n","            best_model = model.state_dict()\n","            torch.save(best_model, checkpoint_path)\n","\n","    print(f'[Val] Best kappa: {best_val_kappa:.4f}, Epoch {best_epoch}')\n","\n","    return model\n","\n","\n","def evaluate_model(model, test_loader, device, test_only=False, prediction_path='./test_predictions.csv'):\n","    model.eval()\n","\n","    all_preds = []\n","    all_labels = []\n","    all_image_ids = []\n","\n","    with tqdm(total=len(test_loader), desc=f'Evaluating', unit=' batch', file=sys.stdout) as pbar:\n","        for i, data in enumerate(test_loader):\n","\n","            if test_only:\n","                images = data\n","            else:\n","                images, labels = data\n","\n","            if not isinstance(images, list):\n","                images = images.to(device)  # single image case\n","            else:\n","                images = [x.to(device) for x in images]  # dual images case\n","\n","            with torch.no_grad():\n","                outputs = model(images)\n","                preds = torch.argmax(outputs, 1)\n","\n","            if not isinstance(images, list):\n","                # single image case\n","                all_preds.extend(preds.cpu().numpy())\n","                image_ids = [\n","                    os.path.basename(test_loader.dataset.data[idx]['img_path']) for idx in\n","                    range(i * test_loader.batch_size, i * test_loader.batch_size + len(images))\n","                ]\n","                all_image_ids.extend(image_ids)\n","                if not test_only:\n","                    all_labels.extend(labels.numpy())\n","            else:\n","                # dual images case\n","                for k in range(2):\n","                    all_preds.extend(preds.cpu().numpy())\n","                    image_ids = [\n","                        os.path.basename(test_loader.dataset.data[idx][f'img_path{k + 1}']) for idx in\n","                        range(i * test_loader.batch_size, i * test_loader.batch_size + len(images[k]))\n","                    ]\n","                    all_image_ids.extend(image_ids)\n","                    if not test_only:\n","                        all_labels.extend(labels.numpy())\n","\n","            pbar.update(1)\n","\n","    # Save predictions to csv file for Kaggle online evaluation\n","    if test_only:\n","        df = pd.DataFrame({\n","            'ID': all_image_ids,\n","            'TARGET': all_preds\n","        })\n","        df.to_csv(prediction_path, index=False)\n","        print(f'[Test] Save predictions to {os.path.abspath(prediction_path)}')\n","    else:\n","        metrics = compute_metrics(all_preds, all_labels)\n","        return metrics\n","\n","\n","def compute_metrics(preds, labels, per_class=False):\n","    kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n","    accuracy = accuracy_score(labels, preds)\n","    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n","    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n","\n","    # Calculate and print precision and recall for each class\n","    if per_class:\n","        precision_per_class = precision_score(labels, preds, average=None, zero_division=0)\n","        recall_per_class = recall_score(labels, preds, average=None, zero_division=0)\n","        return kappa, accuracy, precision, recall, precision_per_class, recall_per_class\n","\n","    return kappa, accuracy, precision, recall\n","\n","\n","class MyModel(nn.Module):\n","    ''' Defines the architecture for fine-tuning.\n","    Uses a pre-trained ResNet18 as the backbone and replaces the final layer\n","    with a custom FC head for 5 classes.'''\n","\n","    def __init__(self, num_classes=5, dropout_rate=0.5):\n","        # This function Configures the backbone and the custom classification head.\n","\n","        super().__init__()\n","\n","        self.backbone = models.resnet18(pretrained=True)\n","        self.backbone.fc = nn.Identity()  # Remove the original classification layer\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(512, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=dropout_rate),\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=dropout_rate),\n","            nn.Linear(128, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        # This function defines how the model processes input data.\n","        x = self.backbone(x)\n","        x = self.fc(x)\n","        return x\n","\n","\n","class MyDualModel(nn.Module):\n","    def __init__(self, num_classes=5, dropout_rate=0.5):\n","        super().__init__()\n","\n","        backbone = models.resnet18(pretrained=True)\n","        backbone.fc = nn.Identity()\n","\n","        # Here the two backbones will have the same structure but unshared weights\n","        self.backbone1 = copy.deepcopy(backbone)\n","        self.backbone2 = copy.deepcopy(backbone)\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(512 * 2, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=dropout_rate),\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=dropout_rate),\n","            nn.Linear(128, num_classes)\n","        )\n","\n","    def forward(self, images):\n","        image1, image2 = images\n","\n","        x1 = self.backbone1(image1)\n","        x2 = self.backbone2(image2)\n","\n","        x = torch.cat((x1, x2), dim=1)\n","        x = self.fc(x)\n","        return x\n","\n","\n","if __name__ == '__main__':\n","    # Choose between 'single image' and 'dual images' pipeline\n","    # This will affect the model definition, dataset pipeline, training and evaluation\n","\n","    mode = 'single'  # forward single image to the model each time\n","    # mode = 'dual'  # forward two images of the same eye to the model and fuse the features\n","\n","    assert mode in ('single', 'dual')\n","\n","    # Define the model\n","    if mode == 'single':\n","        model = MyModel(num_classes=5, dropout_rate=0.5)\n","    else:\n","        model = MyDualModel(num_classes=5, dropout_rate=0.5)\n","\n","    print(model, '\\n')\n","    print('Pipeline Mode:', mode)\n","\n","    # Create datasets\n","    train_dataset = RetinopathyDataset('./DeepDRiD/train.csv', './DeepDRiD/train/', transform_train, mode)\n","    val_dataset = RetinopathyDataset('./DeepDRiD/val.csv', './DeepDRiD/val/', transform_test, mode)\n","    test_dataset = RetinopathyDataset('./DeepDRiD/test.csv', './DeepDRiD/test/', transform_test, mode, test=True)\n","\n","    # Create dataloaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Freeze the backbone layers of the model\n","    for param in model.backbone.parameters():\n","        param.requires_grad = False\n","\n","    # Confirm which layers are trainable\n","    print(\"Trainable Parameters:\")\n","    for name, param in model.named_parameters():\n","        print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")\n","\n","    # Define the weighted CrossEntropyLoss\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Use GPU device is possible\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print('Device:', device)\n","\n","    # Move class weights to the device\n","    model = model.to(device)\n","\n","    # Optimizer and Learning rate scheduler\n","    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","    # Train and evaluate the model with the training and validation set with frozen backbone\n","    model = train_model(\n","        model, train_loader, val_loader, device, criterion, optimizer,\n","        lr_scheduler=lr_scheduler, num_epochs=num_epochs,\n","        checkpoint_path='./model_1_frozen.pth'  # Save the checkpoint for frozen stage\n","    )\n","\n","    # Unfreeze the backbone layers for fine-tuning\n","    for param in model.backbone.parameters():\n","        param.requires_grad = True\n","\n","    # Define a smaller learning rate for fine-tuning\n","    fine_tune_lr = 1e-5\n","    optimizer = torch.optim.Adam(model.parameters(), lr=fine_tune_lr)\n","\n","    # Fine-tune the model\n","    model = train_model(\n","        model=model,\n","        train_loader=train_loader,\n","        val_loader=val_loader,\n","        device=device,\n","        criterion=criterion,\n","        optimizer=optimizer,\n","        lr_scheduler=lr_scheduler,\n","        num_epochs=10,  # Fewer epochs for fine-tuning\n","        checkpoint_path='./model_1_finetuned.pth'\n","    )\n","\n","    # Load the fine-tuned model checkpoint\n","    state_dict = torch.load('./model_1_finetuned.pth', map_location='cpu')\n","    model.load_state_dict(state_dict)\n","\n","    # Save predictions for Kaggle\n","    evaluate_model(\n","        model=model,\n","        test_loader=test_loader,\n","        device=device,\n","        test_only=True,\n","        prediction_path='./sample_submission_finetuned.csv'\n","    )\n","\n","    # Load the pretrained checkpoint\n","    # state_dict = torch.load('./model_1_frozen.pth', map_location='cpu')\n","    # model.load_state_dict(state_dict, strict=True)\n","\n","    # Make predictions on testing set and save the prediction results\n","    #evaluate_model(model, test_loader, device, test_only=True, prediction_path='./sample_submission_frozen.csv')\n"],"metadata":{"id":"E93Z_VuFffHv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735113357257,"user_tz":-60,"elapsed":2497495,"user":{"displayName":"Amir sard","userId":"05229660649413556744"}},"outputId":"0120b9da-4aa6-43f1-8336-182e3aa95547"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 184MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["MyModel(\n","  (backbone): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Identity()\n","  )\n","  (fc): Sequential(\n","    (0): Linear(in_features=512, out_features=256, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=256, out_features=128, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=128, out_features=5, bias=True)\n","  )\n",") \n","\n","Pipeline Mode: single\n","Trainable Parameters:\n","backbone.conv1.weight: Frozen\n","backbone.bn1.weight: Frozen\n","backbone.bn1.bias: Frozen\n","backbone.layer1.0.conv1.weight: Frozen\n","backbone.layer1.0.bn1.weight: Frozen\n","backbone.layer1.0.bn1.bias: Frozen\n","backbone.layer1.0.conv2.weight: Frozen\n","backbone.layer1.0.bn2.weight: Frozen\n","backbone.layer1.0.bn2.bias: Frozen\n","backbone.layer1.1.conv1.weight: Frozen\n","backbone.layer1.1.bn1.weight: Frozen\n","backbone.layer1.1.bn1.bias: Frozen\n","backbone.layer1.1.conv2.weight: Frozen\n","backbone.layer1.1.bn2.weight: Frozen\n","backbone.layer1.1.bn2.bias: Frozen\n","backbone.layer2.0.conv1.weight: Frozen\n","backbone.layer2.0.bn1.weight: Frozen\n","backbone.layer2.0.bn1.bias: Frozen\n","backbone.layer2.0.conv2.weight: Frozen\n","backbone.layer2.0.bn2.weight: Frozen\n","backbone.layer2.0.bn2.bias: Frozen\n","backbone.layer2.0.downsample.0.weight: Frozen\n","backbone.layer2.0.downsample.1.weight: Frozen\n","backbone.layer2.0.downsample.1.bias: Frozen\n","backbone.layer2.1.conv1.weight: Frozen\n","backbone.layer2.1.bn1.weight: Frozen\n","backbone.layer2.1.bn1.bias: Frozen\n","backbone.layer2.1.conv2.weight: Frozen\n","backbone.layer2.1.bn2.weight: Frozen\n","backbone.layer2.1.bn2.bias: Frozen\n","backbone.layer3.0.conv1.weight: Frozen\n","backbone.layer3.0.bn1.weight: Frozen\n","backbone.layer3.0.bn1.bias: Frozen\n","backbone.layer3.0.conv2.weight: Frozen\n","backbone.layer3.0.bn2.weight: Frozen\n","backbone.layer3.0.bn2.bias: Frozen\n","backbone.layer3.0.downsample.0.weight: Frozen\n","backbone.layer3.0.downsample.1.weight: Frozen\n","backbone.layer3.0.downsample.1.bias: Frozen\n","backbone.layer3.1.conv1.weight: Frozen\n","backbone.layer3.1.bn1.weight: Frozen\n","backbone.layer3.1.bn1.bias: Frozen\n","backbone.layer3.1.conv2.weight: Frozen\n","backbone.layer3.1.bn2.weight: Frozen\n","backbone.layer3.1.bn2.bias: Frozen\n","backbone.layer4.0.conv1.weight: Frozen\n","backbone.layer4.0.bn1.weight: Frozen\n","backbone.layer4.0.bn1.bias: Frozen\n","backbone.layer4.0.conv2.weight: Frozen\n","backbone.layer4.0.bn2.weight: Frozen\n","backbone.layer4.0.bn2.bias: Frozen\n","backbone.layer4.0.downsample.0.weight: Frozen\n","backbone.layer4.0.downsample.1.weight: Frozen\n","backbone.layer4.0.downsample.1.bias: Frozen\n","backbone.layer4.1.conv1.weight: Frozen\n","backbone.layer4.1.bn1.weight: Frozen\n","backbone.layer4.1.bn1.bias: Frozen\n","backbone.layer4.1.conv2.weight: Frozen\n","backbone.layer4.1.bn2.weight: Frozen\n","backbone.layer4.1.bn2.bias: Frozen\n","fc.0.weight: Trainable\n","fc.0.bias: Trainable\n","fc.3.weight: Trainable\n","fc.3.bias: Trainable\n","fc.6.weight: Trainable\n","fc.6.bias: Trainable\n","Device: cuda\n","\n","Epoch 1/20\n","Training: 100%|██████████| 50/50 [19:10<00:00, 23.01s/ batch, lr=1.0e-04, Loss=1.5636]\n","[Train] Kappa: 0.0081 Accuracy: 0.2708 Precision: 0.2303 Recall: 0.2708 Loss: 1.5919\n","[Train] Class 0: Precision: 0.3282, Recall: 0.6556\n","[Train] Class 1: Precision: 0.2016, Recall: 0.1042\n","[Train] Class 2: Precision: 0.2000, Recall: 0.1333\n","[Train] Class 3: Precision: 0.2314, Recall: 0.1167\n","[Train] Class 4: Precision: 0.0526, Recall: 0.0333\n","Evaluating: 100%|██████████| 17/17 [06:05<00:00, 21.53s/ batch]\n","[Val] Kappa: 0.0000 Accuracy: 0.3000 Precision: 0.0900 Recall: 0.3000\n","\n","Epoch 2/20\n","Training: 100%|██████████| 50/50 [00:14<00:00,  3.34 batch/s, lr=1.0e-04, Loss=1.4369]\n","[Train] Kappa: 0.1020 Accuracy: 0.3217 Precision: 0.2596 Recall: 0.3217 Loss: 1.5382\n","[Train] Class 0: Precision: 0.3413, Recall: 0.8694\n","[Train] Class 1: Precision: 0.2785, Recall: 0.0917\n","[Train] Class 2: Precision: 0.2500, Recall: 0.0708\n","[Train] Class 3: Precision: 0.2576, Recall: 0.1417\n","[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.45 batch/s]\n","[Val] Kappa: 0.0130 Accuracy: 0.3025 Precision: 0.1905 Recall: 0.3025\n","\n","Epoch 3/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.30 batch/s, lr=1.0e-04, Loss=1.4953]\n","[Train] Kappa: 0.1664 Accuracy: 0.3467 Precision: 0.2964 Recall: 0.3467 Loss: 1.5007\n","[Train] Class 0: Precision: 0.3594, Recall: 0.9333\n","[Train] Class 1: Precision: 0.3864, Recall: 0.0708\n","[Train] Class 2: Precision: 0.2537, Recall: 0.0708\n","[Train] Class 3: Precision: 0.3026, Recall: 0.1917\n","[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  4.14 batch/s]\n","[Val] Kappa: 0.2498 Accuracy: 0.3525 Precision: 0.1872 Recall: 0.3525\n","\n","Epoch 4/20\n","Training: 100%|██████████| 50/50 [00:16<00:00,  3.12 batch/s, lr=1.0e-04, Loss=1.3469]\n","[Train] Kappa: 0.2544 Accuracy: 0.3683 Precision: 0.3821 Recall: 0.3683 Loss: 1.4703\n","[Train] Class 0: Precision: 0.4086, Recall: 0.9194\n","[Train] Class 1: Precision: 0.2254, Recall: 0.0667\n","[Train] Class 2: Precision: 0.2522, Recall: 0.1208\n","[Train] Class 3: Precision: 0.3202, Recall: 0.2708\n","[Train] Class 4: Precision: 1.0000, Recall: 0.0083\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.57 batch/s]\n","[Val] Kappa: 0.4968 Accuracy: 0.4425 Precision: 0.3438 Recall: 0.4425\n","\n","Epoch 5/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.16 batch/s, lr=1.0e-04, Loss=1.4406]\n","[Train] Kappa: 0.3071 Accuracy: 0.3975 Precision: 0.3115 Recall: 0.3975 Loss: 1.4066\n","[Train] Class 0: Precision: 0.4485, Recall: 0.9194\n","[Train] Class 1: Precision: 0.2209, Recall: 0.0792\n","[Train] Class 2: Precision: 0.3182, Recall: 0.1458\n","[Train] Class 3: Precision: 0.3459, Recall: 0.3833\n","[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.91 batch/s]\n","[Val] Kappa: 0.6190 Accuracy: 0.4850 Precision: 0.4065 Recall: 0.4850\n","\n","Epoch 6/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.32 batch/s, lr=1.0e-04, Loss=1.2233]\n","[Train] Kappa: 0.4478 Accuracy: 0.4425 Precision: 0.3614 Recall: 0.4425 Loss: 1.3727\n","[Train] Class 0: Precision: 0.5307, Recall: 0.9361\n","[Train] Class 1: Precision: 0.3333, Recall: 0.1250\n","[Train] Class 2: Precision: 0.3152, Recall: 0.2167\n","[Train] Class 3: Precision: 0.3625, Recall: 0.4667\n","[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.45 batch/s]\n","[Val] Kappa: 0.6155 Accuracy: 0.5000 Precision: 0.4334 Recall: 0.5000\n","\n","Epoch 7/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.18 batch/s, lr=1.0e-04, Loss=1.4567]\n","[Train] Kappa: 0.4896 Accuracy: 0.4550 Precision: 0.4048 Recall: 0.4550 Loss: 1.3195\n","[Train] Class 0: Precision: 0.5626, Recall: 0.9361\n","[Train] Class 1: Precision: 0.3580, Recall: 0.2417\n","[Train] Class 2: Precision: 0.3019, Recall: 0.2000\n","[Train] Class 3: Precision: 0.3704, Recall: 0.4167\n","[Train] Class 4: Precision: 0.3000, Recall: 0.0250\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  4.06 batch/s]\n","[Val] Kappa: 0.6452 Accuracy: 0.5375 Precision: 0.4574 Recall: 0.5375\n","\n","Epoch 8/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.27 batch/s, lr=1.0e-04, Loss=1.3556]\n","[Train] Kappa: 0.5575 Accuracy: 0.4567 Precision: 0.4025 Recall: 0.4567 Loss: 1.2888\n","[Train] Class 0: Precision: 0.6196, Recall: 0.9500\n","[Train] Class 1: Precision: 0.3474, Recall: 0.1375\n","[Train] Class 2: Precision: 0.2260, Recall: 0.1958\n","[Train] Class 3: Precision: 0.3669, Recall: 0.5167\n","[Train] Class 4: Precision: 0.2857, Recall: 0.0167\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.78 batch/s]\n","[Val] Kappa: 0.6655 Accuracy: 0.5425 Precision: 0.4853 Recall: 0.5425\n","\n","Epoch 9/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.18 batch/s, lr=1.0e-04, Loss=1.3736]\n","[Train] Kappa: 0.5326 Accuracy: 0.4608 Precision: 0.4112 Recall: 0.4608 Loss: 1.2643\n","[Train] Class 0: Precision: 0.6200, Recall: 0.9111\n","[Train] Class 1: Precision: 0.3293, Recall: 0.2250\n","[Train] Class 2: Precision: 0.3131, Recall: 0.2583\n","[Train] Class 3: Precision: 0.3584, Recall: 0.4375\n","[Train] Class 4: Precision: 0.2500, Recall: 0.0333\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  4.15 batch/s]\n","[Val] Kappa: 0.6703 Accuracy: 0.5850 Precision: 0.5263 Recall: 0.5850\n","\n","Epoch 10/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.26 batch/s, lr=1.0e-04, Loss=1.1602]\n","[Train] Kappa: 0.5490 Accuracy: 0.4650 Precision: 0.4000 Recall: 0.4650 Loss: 1.2696\n","[Train] Class 0: Precision: 0.6430, Recall: 0.9306\n","[Train] Class 1: Precision: 0.3605, Recall: 0.2208\n","[Train] Class 2: Precision: 0.2767, Recall: 0.2375\n","[Train] Class 3: Precision: 0.3567, Recall: 0.4667\n","[Train] Class 4: Precision: 0.0833, Recall: 0.0083\n","Evaluating: 100%|██████████| 17/17 [00:03<00:00,  4.25 batch/s]\n","[Val] Kappa: 0.6655 Accuracy: 0.5475 Precision: 0.4591 Recall: 0.5475\n","\n","Epoch 11/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.29 batch/s, lr=1.0e-05, Loss=1.0929]\n","[Train] Kappa: 0.5924 Accuracy: 0.4917 Precision: 0.4656 Recall: 0.4917 Loss: 1.2303\n","[Train] Class 0: Precision: 0.6757, Recall: 0.9028\n","[Train] Class 1: Precision: 0.4385, Recall: 0.2375\n","[Train] Class 2: Precision: 0.3164, Recall: 0.3375\n","[Train] Class 3: Precision: 0.3829, Recall: 0.5042\n","[Train] Class 4: Precision: 0.3529, Recall: 0.0500\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.53 batch/s]\n","[Val] Kappa: 0.6715 Accuracy: 0.5575 Precision: 0.5069 Recall: 0.5575\n","\n","Epoch 12/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.19 batch/s, lr=1.0e-05, Loss=1.2796]\n","[Train] Kappa: 0.5998 Accuracy: 0.4942 Precision: 0.4564 Recall: 0.4942 Loss: 1.2126\n","[Train] Class 0: Precision: 0.6830, Recall: 0.9278\n","[Train] Class 1: Precision: 0.4198, Recall: 0.2292\n","[Train] Class 2: Precision: 0.3167, Recall: 0.3167\n","[Train] Class 3: Precision: 0.3820, Recall: 0.5125\n","[Train] Class 4: Precision: 0.2778, Recall: 0.0417\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  4.09 batch/s]\n","[Val] Kappa: 0.6728 Accuracy: 0.5700 Precision: 0.5369 Recall: 0.5700\n","\n","Epoch 13/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.29 batch/s, lr=1.0e-05, Loss=1.1178]\n","[Train] Kappa: 0.6100 Accuracy: 0.4850 Precision: 0.4536 Recall: 0.4850 Loss: 1.2289\n","[Train] Class 0: Precision: 0.6667, Recall: 0.9167\n","[Train] Class 1: Precision: 0.3857, Recall: 0.2250\n","[Train] Class 2: Precision: 0.3142, Recall: 0.2958\n","[Train] Class 3: Precision: 0.3738, Recall: 0.5000\n","[Train] Class 4: Precision: 0.3889, Recall: 0.0583\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.43 batch/s]\n","[Val] Kappa: 0.6767 Accuracy: 0.5600 Precision: 0.5174 Recall: 0.5600\n","\n","Epoch 14/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.19 batch/s, lr=1.0e-05, Loss=1.3288]\n","[Train] Kappa: 0.5748 Accuracy: 0.4950 Precision: 0.4512 Recall: 0.4950 Loss: 1.2117\n","[Train] Class 0: Precision: 0.6829, Recall: 0.9333\n","[Train] Class 1: Precision: 0.4538, Recall: 0.2250\n","[Train] Class 2: Precision: 0.3134, Recall: 0.3708\n","[Train] Class 3: Precision: 0.3813, Recall: 0.4750\n","[Train] Class 4: Precision: 0.1667, Recall: 0.0083\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.97 batch/s]\n","[Val] Kappa: 0.6717 Accuracy: 0.5675 Precision: 0.5176 Recall: 0.5675\n","\n","Epoch 15/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.33 batch/s, lr=1.0e-05, Loss=0.9942]\n","[Train] Kappa: 0.5908 Accuracy: 0.4858 Precision: 0.4604 Recall: 0.4858 Loss: 1.2229\n","[Train] Class 0: Precision: 0.6880, Recall: 0.9250\n","[Train] Class 1: Precision: 0.3681, Recall: 0.2500\n","[Train] Class 2: Precision: 0.2987, Recall: 0.2875\n","[Train] Class 3: Precision: 0.3722, Recall: 0.4792\n","[Train] Class 4: Precision: 0.4615, Recall: 0.0500\n","Evaluating: 100%|██████████| 17/17 [00:05<00:00,  3.28 batch/s]\n","[Val] Kappa: 0.6804 Accuracy: 0.5700 Precision: 0.5259 Recall: 0.5700\n","\n","Epoch 16/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.25 batch/s, lr=1.0e-05, Loss=1.1388]\n","[Train] Kappa: 0.5802 Accuracy: 0.4750 Precision: 0.4242 Recall: 0.4750 Loss: 1.2063\n","[Train] Class 0: Precision: 0.6601, Recall: 0.9333\n","[Train] Class 1: Precision: 0.3919, Recall: 0.2417\n","[Train] Class 2: Precision: 0.3059, Recall: 0.2792\n","[Train] Class 3: Precision: 0.3419, Recall: 0.4458\n","[Train] Class 4: Precision: 0.1818, Recall: 0.0167\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  4.18 batch/s]\n","[Val] Kappa: 0.6740 Accuracy: 0.5650 Precision: 0.5175 Recall: 0.5650\n","\n","Epoch 17/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.23 batch/s, lr=1.0e-05, Loss=0.9364]\n","[Train] Kappa: 0.5667 Accuracy: 0.5058 Precision: 0.4839 Recall: 0.5058 Loss: 1.2065\n","[Train] Class 0: Precision: 0.6728, Recall: 0.9139\n","[Train] Class 1: Precision: 0.4437, Recall: 0.2792\n","[Train] Class 2: Precision: 0.3506, Recall: 0.3375\n","[Train] Class 3: Precision: 0.3937, Recall: 0.5250\n","[Train] Class 4: Precision: 0.4444, Recall: 0.0333\n","Evaluating: 100%|██████████| 17/17 [00:05<00:00,  3.36 batch/s]\n","[Val] Kappa: 0.6771 Accuracy: 0.5850 Precision: 0.5419 Recall: 0.5850\n","\n","Epoch 18/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.26 batch/s, lr=1.0e-05, Loss=1.3640]\n","[Train] Kappa: 0.5764 Accuracy: 0.4858 Precision: 0.4509 Recall: 0.4858 Loss: 1.2230\n","[Train] Class 0: Precision: 0.6618, Recall: 0.8861\n","[Train] Class 1: Precision: 0.4286, Recall: 0.2750\n","[Train] Class 2: Precision: 0.3145, Recall: 0.3250\n","[Train] Class 3: Precision: 0.3854, Recall: 0.4833\n","[Train] Class 4: Precision: 0.2667, Recall: 0.0333\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  4.20 batch/s]\n","[Val] Kappa: 0.6562 Accuracy: 0.5650 Precision: 0.4966 Recall: 0.5650\n","\n","Epoch 19/20\n","Training: 100%|██████████| 50/50 [00:14<00:00,  3.34 batch/s, lr=1.0e-05, Loss=1.0864]\n","[Train] Kappa: 0.5788 Accuracy: 0.4858 Precision: 0.4558 Recall: 0.4858 Loss: 1.2205\n","[Train] Class 0: Precision: 0.6680, Recall: 0.9111\n","[Train] Class 1: Precision: 0.3929, Recall: 0.2750\n","[Train] Class 2: Precision: 0.3022, Recall: 0.2833\n","[Train] Class 3: Precision: 0.3821, Recall: 0.4792\n","[Train] Class 4: Precision: 0.4000, Recall: 0.0500\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.91 batch/s]\n","[Val] Kappa: 0.6740 Accuracy: 0.5725 Precision: 0.5256 Recall: 0.5725\n","\n","Epoch 20/20\n","Training: 100%|██████████| 50/50 [00:15<00:00,  3.17 batch/s, lr=1.0e-05, Loss=1.0427]\n","[Train] Kappa: 0.6002 Accuracy: 0.4983 Precision: 0.4522 Recall: 0.4983 Loss: 1.1995\n","[Train] Class 0: Precision: 0.6803, Recall: 0.9222\n","[Train] Class 1: Precision: 0.3681, Recall: 0.2500\n","[Train] Class 2: Precision: 0.3364, Recall: 0.3083\n","[Train] Class 3: Precision: 0.4110, Recall: 0.5292\n","[Train] Class 4: Precision: 0.2500, Recall: 0.0417\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.91 batch/s]\n","[Val] Kappa: 0.6824 Accuracy: 0.5775 Precision: 0.5303 Recall: 0.5775\n","[Val] Best kappa: 0.6824, Epoch 20\n","\n","Epoch 1/10\n","Training: 100%|██████████| 50/50 [00:17<00:00,  2.82 batch/s, lr=1.0e-05, Loss=1.1593]\n","[Train] Kappa: 0.6195 Accuracy: 0.4883 Precision: 0.4483 Recall: 0.4883 Loss: 1.1822\n","[Train] Class 0: Precision: 0.6988, Recall: 0.9472\n","[Train] Class 1: Precision: 0.3800, Recall: 0.2375\n","[Train] Class 2: Precision: 0.2977, Recall: 0.2667\n","[Train] Class 3: Precision: 0.3595, Recall: 0.4958\n","[Train] Class 4: Precision: 0.3125, Recall: 0.0417\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.49 batch/s]\n","[Val] Kappa: 0.7082 Accuracy: 0.5825 Precision: 0.5255 Recall: 0.5825\n","\n","Epoch 2/10\n","Training: 100%|██████████| 50/50 [00:17<00:00,  2.83 batch/s, lr=1.0e-05, Loss=1.2002]\n","[Train] Kappa: 0.6485 Accuracy: 0.5133 Precision: 0.4824 Recall: 0.5133 Loss: 1.1518\n","[Train] Class 0: Precision: 0.7287, Recall: 0.9250\n","[Train] Class 1: Precision: 0.4459, Recall: 0.2917\n","[Train] Class 2: Precision: 0.3407, Recall: 0.3208\n","[Train] Class 3: Precision: 0.3844, Recall: 0.5333\n","[Train] Class 4: Precision: 0.2963, Recall: 0.0667\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  4.12 batch/s]\n","[Val] Kappa: 0.7190 Accuracy: 0.5875 Precision: 0.5258 Recall: 0.5875\n","\n","Epoch 3/10\n","Training: 100%|██████████| 50/50 [00:18<00:00,  2.74 batch/s, lr=1.0e-05, Loss=1.0237]\n","[Train] Kappa: 0.6875 Accuracy: 0.5508 Precision: 0.5284 Recall: 0.5508 Loss: 1.1028\n","[Train] Class 0: Precision: 0.7598, Recall: 0.9667\n","[Train] Class 1: Precision: 0.5342, Recall: 0.3250\n","[Train] Class 2: Precision: 0.3857, Recall: 0.3583\n","[Train] Class 3: Precision: 0.4023, Recall: 0.5833\n","[Train] Class 4: Precision: 0.3600, Recall: 0.0750\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.72 batch/s]\n","[Val] Kappa: 0.7368 Accuracy: 0.5875 Precision: 0.5414 Recall: 0.5875\n","\n","Epoch 4/10\n","Training: 100%|██████████| 50/50 [00:17<00:00,  2.81 batch/s, lr=1.0e-05, Loss=1.0039]\n","[Train] Kappa: 0.6769 Accuracy: 0.5342 Precision: 0.5100 Recall: 0.5342 Loss: 1.0892\n","[Train] Class 0: Precision: 0.7828, Recall: 0.9611\n","[Train] Class 1: Precision: 0.4494, Recall: 0.3333\n","[Train] Class 2: Precision: 0.3398, Recall: 0.2917\n","[Train] Class 3: Precision: 0.3866, Recall: 0.5542\n","[Train] Class 4: Precision: 0.4000, Recall: 0.1000\n","Evaluating: 100%|██████████| 17/17 [00:05<00:00,  3.34 batch/s]\n","[Val] Kappa: 0.7126 Accuracy: 0.6025 Precision: 0.5317 Recall: 0.6025\n","\n","Epoch 5/10\n","Training: 100%|██████████| 50/50 [00:17<00:00,  2.83 batch/s, lr=1.0e-05, Loss=0.9936]\n","[Train] Kappa: 0.7216 Accuracy: 0.5675 Precision: 0.5434 Recall: 0.5675 Loss: 1.0695\n","[Train] Class 0: Precision: 0.7854, Recall: 0.9556\n","[Train] Class 1: Precision: 0.5028, Recall: 0.3708\n","[Train] Class 2: Precision: 0.3756, Recall: 0.3083\n","[Train] Class 3: Precision: 0.4510, Recall: 0.6708\n","[Train] Class 4: Precision: 0.4194, Recall: 0.1083\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  4.07 batch/s]\n","[Val] Kappa: 0.7209 Accuracy: 0.6125 Precision: 0.5439 Recall: 0.6125\n","\n","Epoch 6/10\n","Training: 100%|██████████| 50/50 [00:17<00:00,  2.80 batch/s, lr=1.0e-05, Loss=1.1093]\n","[Train] Kappa: 0.7234 Accuracy: 0.5317 Precision: 0.5009 Recall: 0.5317 Loss: 1.0724\n","[Train] Class 0: Precision: 0.7800, Recall: 0.9556\n","[Train] Class 1: Precision: 0.4481, Recall: 0.3417\n","[Train] Class 2: Precision: 0.3096, Recall: 0.2542\n","[Train] Class 3: Precision: 0.4066, Recall: 0.5625\n","[Train] Class 4: Precision: 0.3404, Recall: 0.1333\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.67 batch/s]\n","[Val] Kappa: 0.7633 Accuracy: 0.6300 Precision: 0.5701 Recall: 0.6300\n","\n","Epoch 7/10\n","Training: 100%|██████████| 50/50 [00:17<00:00,  2.89 batch/s, lr=1.0e-05, Loss=0.9640]\n","[Train] Kappa: 0.7500 Accuracy: 0.5917 Precision: 0.5594 Recall: 0.5917 Loss: 1.0253\n","[Train] Class 0: Precision: 0.8144, Recall: 0.9750\n","[Train] Class 1: Precision: 0.5500, Recall: 0.4583\n","[Train] Class 2: Precision: 0.4093, Recall: 0.3667\n","[Train] Class 3: Precision: 0.4734, Recall: 0.6292\n","[Train] Class 4: Precision: 0.2857, Recall: 0.0833\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  3.93 batch/s]\n","[Val] Kappa: 0.7332 Accuracy: 0.6250 Precision: 0.5536 Recall: 0.6250\n","\n","Epoch 8/10\n","Training: 100%|██████████| 50/50 [00:18<00:00,  2.72 batch/s, lr=1.0e-05, Loss=0.9790]\n","[Train] Kappa: 0.7437 Accuracy: 0.5567 Precision: 0.5204 Recall: 0.5567 Loss: 1.0226\n","[Train] Class 0: Precision: 0.8055, Recall: 0.9778\n","[Train] Class 1: Precision: 0.4864, Recall: 0.4458\n","[Train] Class 2: Precision: 0.3005, Recall: 0.2292\n","[Train] Class 3: Precision: 0.4437, Recall: 0.5750\n","[Train] Class 4: Precision: 0.3265, Recall: 0.1333\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  4.25 batch/s]\n","[Val] Kappa: 0.7404 Accuracy: 0.6225 Precision: 0.5526 Recall: 0.6225\n","\n","Epoch 9/10\n","Training: 100%|██████████| 50/50 [00:17<00:00,  2.91 batch/s, lr=1.0e-05, Loss=0.8501]\n","[Train] Kappa: 0.7684 Accuracy: 0.5925 Precision: 0.5671 Recall: 0.5925 Loss: 1.0191\n","[Train] Class 0: Precision: 0.8079, Recall: 0.9694\n","[Train] Class 1: Precision: 0.5773, Recall: 0.5292\n","[Train] Class 2: Precision: 0.3750, Recall: 0.2375\n","[Train] Class 3: Precision: 0.4504, Recall: 0.6625\n","[Train] Class 4: Precision: 0.4419, Recall: 0.1583\n","Evaluating: 100%|██████████| 17/17 [00:05<00:00,  3.37 batch/s]\n","[Val] Kappa: 0.7462 Accuracy: 0.6225 Precision: 0.5564 Recall: 0.6225\n","\n","Epoch 10/10\n","Training: 100%|██████████| 50/50 [00:17<00:00,  2.89 batch/s, lr=1.0e-05, Loss=1.2483]\n","[Train] Kappa: 0.7721 Accuracy: 0.5792 Precision: 0.5529 Recall: 0.5792 Loss: 1.0007\n","[Train] Class 0: Precision: 0.8310, Recall: 0.9694\n","[Train] Class 1: Precision: 0.5185, Recall: 0.4667\n","[Train] Class 2: Precision: 0.3772, Recall: 0.2625\n","[Train] Class 3: Precision: 0.4448, Recall: 0.6208\n","[Train] Class 4: Precision: 0.3548, Recall: 0.1833\n","Evaluating: 100%|██████████| 17/17 [00:04<00:00,  4.22 batch/s]\n","[Val] Kappa: 0.7487 Accuracy: 0.6325 Precision: 0.5752 Recall: 0.6325\n","[Val] Best kappa: 0.7633, Epoch 6\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-eed39a2f8e18>:455: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load('./model_1_finetuned.pth', map_location='cpu')\n"]},{"output_type":"stream","name":"stdout","text":["Evaluating: 100%|██████████| 17/17 [06:08<00:00, 21.70s/ batch]\n","[Test] Save predictions to /content/drive/MyDrive/Colab Notebooks/DeepLearning_Final_Project_2024/sample_submission_finetuned.csv\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python (dlcourseenv)","language":"python","name":"dlcourseenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}