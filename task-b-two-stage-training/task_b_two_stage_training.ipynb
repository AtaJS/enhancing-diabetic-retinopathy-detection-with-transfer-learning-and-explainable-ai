{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","authorship_tag":"ABX9TyOqnHb1I9Appn50NN7leaQq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import copy\n","import os\n","import random\n","import sys\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from PIL import Image\n","from sklearn.metrics import cohen_kappa_score, precision_score, recall_score, accuracy_score\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models, transforms\n","from torchvision.transforms.functional import to_pil_image\n","from tqdm import tqdm\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6OMsmuSSGMr","executionInfo":{"status":"ok","timestamp":1735807310362,"user_tz":-120,"elapsed":31711,"user":{"displayName":"Amir sard","userId":"05229660649413556744"}},"outputId":"a0be8972-24b9-4922-d494-68c3538eddf9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/DeepLearning_Final_Project_2024')\n","print(os.listdir('DeepDRiD'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjmfPmOpSTaU","executionInfo":{"status":"ok","timestamp":1735807312525,"user_tz":-120,"elapsed":526,"user":{"displayName":"Amir sard","userId":"05229660649413556744"}},"outputId":"82da0b5b-c50d-4752-8100-48e12f72e205"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['sample_submission.csv', 'test.csv', 'train.csv', 'val.csv', 'test', 'train', 'val']\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"UgzydvhsKgiA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735759025022,"user_tz":-120,"elapsed":300391,"user":{"displayName":"Amir sard","userId":"05229660649413556744"}},"outputId":"a6ec67cf-e92f-4823-8125-f1f2c21f79f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using pretrained efficientnet_b0 backbone for Task (b)...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n","<ipython-input-7-43dde9671f64>:321: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load('./pretrained/efficientnet_b0.pth')\n","<ipython-input-7-43dde9671f64>:394: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(pretrained_weights_path)  # Load weights\n"]},{"output_type":"stream","name":"stdout","text":["missing keys: ['classifier.1.weight', 'classifier.1.bias']\n","unexpected keys: []\n","Missing keys: ['classifier.1.weight', 'classifier.1.bias', 'fc.0.weight', 'fc.0.bias', 'fc.3.weight', 'fc.3.bias', 'fc.6.weight', 'fc.6.bias'], Unexpected keys: []\n","\n","Epoch 1/10\n","Training: 100%|██████████| 50/50 [00:23<00:00,  2.15 batch/s, lr=1.0e-04, Loss=4.3899]\n","[Train] Kappa: 0.0080 Accuracy: 0.4150 Precision: 0.5384 Recall: 0.4150 Loss: 5.8356\n","[Train] Class 0: Precision: 0.6575, Recall: 0.7306\n","[Train] Class 1: Precision: 0.5714, Recall: 0.3667\n","[Train] Class 2: Precision: 0.5169, Recall: 0.1917\n","[Train] Class 3: Precision: 0.4925, Recall: 0.4125\n","[Train] Class 4: Precision: 0.2500, Recall: 0.0167\n","[Train] Class 5: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 6: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 7: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 8: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 9: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 10: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 11: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 12: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 13: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 14: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 15: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 16: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 17: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 18: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 19: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 20: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 21: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 22: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 23: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 24: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 25: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 26: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 27: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 28: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 29: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 30: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 31: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 32: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 33: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 34: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 35: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 36: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 37: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 38: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 39: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 40: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 41: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 42: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 43: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 44: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 45: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 46: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 47: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 48: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 49: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 50: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 51: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 52: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 53: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 54: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 55: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 56: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 57: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 58: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 59: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 60: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 61: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 62: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 63: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 64: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 65: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 66: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 67: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 68: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 69: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 70: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 71: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 72: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 73: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 74: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 75: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 76: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 77: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 78: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 79: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 80: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 81: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 82: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 83: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 84: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 85: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 86: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 87: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 88: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 89: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 90: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 91: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 92: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 93: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 94: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 95: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 96: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 97: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 98: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 99: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 100: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 101: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 102: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 103: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 104: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 105: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 106: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 107: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 108: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 109: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 110: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 111: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 112: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 113: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 114: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 115: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 116: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 117: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 118: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 119: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 120: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 121: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 122: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 123: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 124: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 125: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 126: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 127: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 128: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 129: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 130: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 131: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 132: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 133: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 134: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 135: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 136: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 137: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 138: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 139: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 140: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 141: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 142: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 143: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 144: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 145: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 146: Precision: 0.0000, Recall: 0.0000\n","[Train] Class 147: Precision: 0.0000, Recall: 0.0000\n","Evaluating: 100%|██████████| 17/17 [00:07<00:00,  2.31 batch/s]\n","[Val] Kappa: 0.7816 Accuracy: 0.6650 Precision: 0.6048 Recall: 0.6650\n","\n","Epoch 2/10\n","Training: 100%|██████████| 50/50 [00:22<00:00,  2.20 batch/s, lr=1.0e-04, Loss=1.0982]\n","[Train] Kappa: 0.7793 Accuracy: 0.6508 Precision: 0.5858 Recall: 0.6508 Loss: 2.5772\n","[Train] Class 0: Precision: 0.7473, Recall: 0.9778\n","[Train] Class 1: Precision: 0.6266, Recall: 0.6083\n","[Train] Class 2: Precision: 0.6290, Recall: 0.3250\n","[Train] Class 3: Precision: 0.5526, Recall: 0.8542\n","[Train] Class 4: Precision: 0.0000, Recall: 0.0000\n","Evaluating: 100%|██████████| 17/17 [00:07<00:00,  2.38 batch/s]\n","[Val] Kappa: 0.8078 Accuracy: 0.7050 Precision: 0.6318 Recall: 0.7050\n","\n","Epoch 3/10\n","Training: 100%|██████████| 50/50 [00:20<00:00,  2.42 batch/s, lr=1.0e-04, Loss=0.9631]\n","[Train] Kappa: 0.8154 Accuracy: 0.6933 Precision: 0.6791 Recall: 0.6933 Loss: 1.0345\n","[Train] Class 0: Precision: 0.8301, Recall: 0.9639\n","[Train] Class 1: Precision: 0.6898, Recall: 0.7042\n","[Train] Class 2: Precision: 0.5529, Recall: 0.4792\n","[Train] Class 3: Precision: 0.6124, Recall: 0.7833\n","[Train] Class 4: Precision: 0.5909, Recall: 0.1083\n","Evaluating: 100%|██████████| 17/17 [00:08<00:00,  2.10 batch/s]\n","[Val] Kappa: 0.8108 Accuracy: 0.6925 Precision: 0.6568 Recall: 0.6925\n","\n","Epoch 4/10\n","Training: 100%|██████████| 50/50 [00:22<00:00,  2.20 batch/s, lr=1.0e-04, Loss=0.7662]\n","[Train] Kappa: 0.8460 Accuracy: 0.6900 Precision: 0.6834 Recall: 0.6900 Loss: 0.8743\n","[Train] Class 0: Precision: 0.8479, Recall: 0.9444\n","[Train] Class 1: Precision: 0.6906, Recall: 0.6417\n","[Train] Class 2: Precision: 0.5274, Recall: 0.5208\n","[Train] Class 3: Precision: 0.6145, Recall: 0.7042\n","[Train] Class 4: Precision: 0.6250, Recall: 0.3333\n","Evaluating: 100%|██████████| 17/17 [00:07<00:00,  2.39 batch/s]\n","[Val] Kappa: 0.8021 Accuracy: 0.7075 Precision: 0.6851 Recall: 0.7075\n","\n","Epoch 5/10\n","Training: 100%|██████████| 50/50 [00:21<00:00,  2.29 batch/s, lr=1.0e-04, Loss=0.7423]\n","[Train] Kappa: 0.8627 Accuracy: 0.7450 Precision: 0.7399 Recall: 0.7450 Loss: 0.7234\n","[Train] Class 0: Precision: 0.8903, Recall: 0.9694\n","[Train] Class 1: Precision: 0.7208, Recall: 0.7208\n","[Train] Class 2: Precision: 0.6167, Recall: 0.5833\n","[Train] Class 3: Precision: 0.6751, Recall: 0.7792\n","[Train] Class 4: Precision: 0.7031, Recall: 0.3750\n","Evaluating: 100%|██████████| 17/17 [00:05<00:00,  3.25 batch/s]\n","[Val] Kappa: 0.8062 Accuracy: 0.7025 Precision: 0.6786 Recall: 0.7025\n","\n","Epoch 6/10\n","Training: 100%|██████████| 50/50 [00:20<00:00,  2.41 batch/s, lr=1.0e-04, Loss=0.4376]\n","[Train] Kappa: 0.8736 Accuracy: 0.7367 Precision: 0.7316 Recall: 0.7367 Loss: 0.6911\n","[Train] Class 0: Precision: 0.8943, Recall: 0.9639\n","[Train] Class 1: Precision: 0.7639, Recall: 0.7417\n","[Train] Class 2: Precision: 0.5477, Recall: 0.5500\n","[Train] Class 3: Precision: 0.6757, Recall: 0.7292\n","[Train] Class 4: Precision: 0.6582, Recall: 0.4333\n","Evaluating: 100%|██████████| 17/17 [00:06<00:00,  2.46 batch/s]\n","[Val] Kappa: 0.7990 Accuracy: 0.7000 Precision: 0.6776 Recall: 0.7000\n","\n","Epoch 7/10\n","Training: 100%|██████████| 50/50 [00:21<00:00,  2.31 batch/s, lr=1.0e-04, Loss=0.8199]\n","[Train] Kappa: 0.8892 Accuracy: 0.7792 Precision: 0.7735 Recall: 0.7792 Loss: 0.6105\n","[Train] Class 0: Precision: 0.9003, Recall: 0.9778\n","[Train] Class 1: Precision: 0.7801, Recall: 0.7833\n","[Train] Class 2: Precision: 0.6502, Recall: 0.6042\n","[Train] Class 3: Precision: 0.7255, Recall: 0.7708\n","[Train] Class 4: Precision: 0.7222, Recall: 0.5417\n","Evaluating: 100%|██████████| 17/17 [00:06<00:00,  2.63 batch/s]\n","[Val] Kappa: 0.8110 Accuracy: 0.7000 Precision: 0.6880 Recall: 0.7000\n","\n","Epoch 8/10\n","Training: 100%|██████████| 50/50 [00:21<00:00,  2.28 batch/s, lr=1.0e-04, Loss=0.4591]\n","[Train] Kappa: 0.8939 Accuracy: 0.7892 Precision: 0.7892 Recall: 0.7892 Loss: 0.5958\n","[Train] Class 0: Precision: 0.9101, Recall: 0.9556\n","[Train] Class 1: Precision: 0.7958, Recall: 0.7958\n","[Train] Class 2: Precision: 0.6628, Recall: 0.7125\n","[Train] Class 3: Precision: 0.7328, Recall: 0.7542\n","[Train] Class 4: Precision: 0.7792, Recall: 0.5000\n","Evaluating: 100%|██████████| 17/17 [00:07<00:00,  2.16 batch/s]\n","[Val] Kappa: 0.8138 Accuracy: 0.7000 Precision: 0.6877 Recall: 0.7000\n","\n","Epoch 9/10\n","Training: 100%|██████████| 50/50 [00:21<00:00,  2.33 batch/s, lr=1.0e-04, Loss=0.5983]\n","[Train] Kappa: 0.9028 Accuracy: 0.8058 Precision: 0.8014 Recall: 0.8058 Loss: 0.5436\n","[Train] Class 0: Precision: 0.9193, Recall: 0.9806\n","[Train] Class 1: Precision: 0.8140, Recall: 0.8208\n","[Train] Class 2: Precision: 0.6820, Recall: 0.6792\n","[Train] Class 3: Precision: 0.7815, Recall: 0.7750\n","[Train] Class 4: Precision: 0.7010, Recall: 0.5667\n","Evaluating: 100%|██████████| 17/17 [00:06<00:00,  2.76 batch/s]\n","[Val] Kappa: 0.8314 Accuracy: 0.7150 Precision: 0.7022 Recall: 0.7150\n","\n","Epoch 10/10\n","Training: 100%|██████████| 50/50 [00:21<00:00,  2.35 batch/s, lr=1.0e-04, Loss=0.3138]\n","[Train] Kappa: 0.8943 Accuracy: 0.7967 Precision: 0.7934 Recall: 0.7967 Loss: 0.5327\n","[Train] Class 0: Precision: 0.9084, Recall: 0.9639\n","[Train] Class 1: Precision: 0.7975, Recall: 0.7875\n","[Train] Class 2: Precision: 0.6765, Recall: 0.6708\n","[Train] Class 3: Precision: 0.7597, Recall: 0.8167\n","[Train] Class 4: Precision: 0.7412, Recall: 0.5250\n","Evaluating: 100%|██████████| 17/17 [00:05<00:00,  2.90 batch/s]"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-7-43dde9671f64>:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load('./model_task_b_finetuned_efficientnet_b0.pth')\n"]},{"output_type":"stream","name":"stdout","text":["\n","[Val] Kappa: 0.8220 Accuracy: 0.7050 Precision: 0.6923 Recall: 0.7050\n","[Val] Best kappa: 0.8314, Epoch 9\n","Saving test set predictions...\n","Evaluating: 100%|██████████| 17/17 [00:09<00:00,  1.74 batch/s]\n","[Test] Save predictions to /content/drive/MyDrive/Colab Notebooks/DeepLearning_Final_Project_2024/sample_submission_task_b_efficientnet_b0.csv\n","Task (b) complete! Predictions saved to './sample_submission_task_b_efficientnet_b0.csv'.\n"]}],"source":["# Hyper Parameters\n","batch_size = 24\n","num_classes = 5  # 5 DR levels\n","learning_rate = 0.0001\n","num_epochs = 20\n","\n","\n","class RetinopathyDataset(Dataset):\n","    def __init__(self, ann_file, image_dir, transform=None, mode='single', test=False):\n","        self.ann_file = ann_file\n","        self.image_dir = image_dir\n","        self.transform = transform\n","\n","        self.test = test\n","        self.mode = mode\n","\n","        if self.mode == 'single':\n","            self.data = self.load_data()\n","        else:\n","            self.data = self.load_data_dual()\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        if self.mode == 'single':\n","            return self.get_item(index)\n","        else:\n","            return self.get_item_dual(index)\n","\n","    # 1. single image\n","    def load_data(self):\n","        df = pd.read_csv(self.ann_file)\n","\n","        data = []\n","        for _, row in df.iterrows():\n","            file_info = dict()\n","            file_info['img_path'] = os.path.join(self.image_dir, row['img_path'])\n","            if not self.test:\n","                file_info['dr_level'] = int(row['patient_DR_Level'])\n","            data.append(file_info)\n","        return data\n","\n","    def get_item(self, index):\n","        data = self.data[index]\n","        img = Image.open(data['img_path']).convert('RGB')\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        if not self.test:\n","            label = torch.tensor(data['dr_level'], dtype=torch.int64)\n","            return img, label\n","        else:\n","            return img\n","\n","    # 2. dual image\n","    def load_data_dual(self):\n","        df = pd.read_csv(self.ann_file)\n","\n","        df['prefix'] = df['image_id'].str.split('_').str[0]  # The patient id of each image\n","        df['suffix'] = df['image_id'].str.split('_').str[1].str[0]  # The left or right eye\n","        grouped = df.groupby(['prefix', 'suffix'])\n","\n","        data = []\n","        for (prefix, suffix), group in grouped:\n","            file_info = dict()\n","            file_info['img_path1'] = os.path.join(self.image_dir, group.iloc[0]['img_path'])\n","            file_info['img_path2'] = os.path.join(self.image_dir, group.iloc[1]['img_path'])\n","            if not self.test:\n","                file_info['dr_level'] = int(group.iloc[0]['patient_DR_Level'])\n","            data.append(file_info)\n","        return data\n","\n","    def get_item_dual(self, index):\n","        data = self.data[index]\n","        img1 = Image.open(data['img_path1']).convert('RGB')\n","        img2 = Image.open(data['img_path2']).convert('RGB')\n","\n","        if self.transform:\n","            img1 = self.transform(img1)\n","            img2 = self.transform(img2)\n","\n","        if not self.test:\n","            label = torch.tensor(data['dr_level'], dtype=torch.int64)\n","            return [img1, img2], label\n","        else:\n","            return [img1, img2]\n","\n","\n","class CutOut(object):\n","    def __init__(self, mask_size, p=0.5):\n","        self.mask_size = mask_size\n","        self.p = p\n","\n","    def __call__(self, img):\n","        if np.random.rand() > self.p:\n","            return img\n","\n","        # Ensure the image is a tensor\n","        if not isinstance(img, torch.Tensor):\n","            raise TypeError('Input image must be a torch.Tensor')\n","\n","        # Get height and width of the image\n","        h, w = img.shape[1], img.shape[2]\n","        mask_size_half = self.mask_size // 2\n","        offset = 1 if self.mask_size % 2 == 0 else 0\n","\n","        cx = np.random.randint(mask_size_half, w + offset - mask_size_half)\n","        cy = np.random.randint(mask_size_half, h + offset - mask_size_half)\n","\n","        xmin, xmax = cx - mask_size_half, cx + mask_size_half + offset\n","        ymin, ymax = cy - mask_size_half, cy + mask_size_half + offset\n","        xmin, xmax = max(0, xmin), min(w, xmax)\n","        ymin, ymax = max(0, ymin), min(h, ymax)\n","\n","        img[:, ymin:ymax, xmin:xmax] = 0\n","        return img\n","\n","\n","class SLORandomPad:\n","    def __init__(self, size):\n","        self.size = size\n","\n","    def __call__(self, img):\n","        pad_width = max(0, self.size[0] - img.width)\n","        pad_height = max(0, self.size[1] - img.height)\n","        pad_left = random.randint(0, pad_width)\n","        pad_top = random.randint(0, pad_height)\n","        pad_right = pad_width - pad_left\n","        pad_bottom = pad_height - pad_top\n","        return transforms.functional.pad(img, (pad_left, pad_top, pad_right, pad_bottom))\n","\n","\n","class FundRandomRotate:\n","    def __init__(self, prob, degree):\n","        self.prob = prob\n","        self.degree = degree\n","\n","    def __call__(self, img):\n","        if random.random() < self.prob:\n","            angle = random.uniform(-self.degree, self.degree)\n","            return transforms.functional.rotate(img, angle)\n","        return img\n","\n","\n","transform_train = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.RandomCrop((210, 210)),\n","    SLORandomPad((224, 224)),\n","    FundRandomRotate(prob=0.5, degree=30),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomVerticalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=(0.1, 0.9)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","def train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=25,\n","                checkpoint_path='model.pth'):\n","    best_model = model.state_dict()\n","    best_epoch = None\n","    best_val_kappa = -1.0  # Initialize the best kappa score\n","\n","    for epoch in range(1, num_epochs + 1):\n","        print(f'\\nEpoch {epoch}/{num_epochs}')\n","        running_loss = []\n","        all_preds = []\n","        all_labels = []\n","\n","        model.train()\n","\n","        with tqdm(total=len(train_loader), desc=f'Training', unit=' batch', file=sys.stdout) as pbar:\n","            for images, labels in train_loader:\n","                if not isinstance(images, list):\n","                    images = images.to(device)  # single image case\n","                else:\n","                    images = [x.to(device) for x in images]  # dual images case\n","\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                outputs = model(images)\n","                loss = criterion(outputs, labels.long())\n","\n","                loss.backward()\n","                optimizer.step()\n","\n","                preds = torch.argmax(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","                running_loss.append(loss.item())\n","\n","                pbar.set_postfix({'lr': f'{optimizer.param_groups[0][\"lr\"]:.1e}', 'Loss': f'{loss.item():.4f}'})\n","                pbar.update(1)\n","\n","        lr_scheduler.step()\n","\n","        epoch_loss = sum(running_loss) / len(running_loss)\n","\n","        train_metrics = compute_metrics(all_preds, all_labels, per_class=True)\n","        kappa, accuracy, precision, recall = train_metrics[:4]\n","\n","        print(f'[Train] Kappa: {kappa:.4f} Accuracy: {accuracy:.4f} '\n","              f'Precision: {precision:.4f} Recall: {recall:.4f} Loss: {epoch_loss:.4f}')\n","\n","        if len(train_metrics) > 4:\n","            precision_per_class, recall_per_class = train_metrics[4:]\n","            for i, (precision, recall) in enumerate(zip(precision_per_class, recall_per_class)):\n","                print(f'[Train] Class {i}: Precision: {precision:.4f}, Recall: {recall:.4f}')\n","\n","        # Evaluation on the validation set at the end of each epoch\n","        val_metrics = evaluate_model(model, val_loader, device)\n","        val_kappa, val_accuracy, val_precision, val_recall = val_metrics[:4]\n","        print(f'[Val] Kappa: {val_kappa:.4f} Accuracy: {val_accuracy:.4f} '\n","              f'Precision: {val_precision:.4f} Recall: {val_recall:.4f}')\n","\n","        if val_kappa > best_val_kappa:\n","            best_val_kappa = val_kappa\n","            best_epoch = epoch\n","            best_model = model.state_dict()\n","            torch.save(best_model, checkpoint_path)\n","\n","    print(f'[Val] Best kappa: {best_val_kappa:.4f}, Epoch {best_epoch}')\n","\n","    return model\n","\n","\n","def evaluate_model(model, test_loader, device, test_only=False, prediction_path='./test_predictions.csv'):\n","    model.eval()\n","\n","    all_preds = []\n","    all_labels = []\n","    all_image_ids = []\n","\n","    with tqdm(total=len(test_loader), desc=f'Evaluating', unit=' batch', file=sys.stdout) as pbar:\n","        for i, data in enumerate(test_loader):\n","\n","            if test_only:\n","                images = data\n","            else:\n","                images, labels = data\n","\n","            if not isinstance(images, list):\n","                images = images.to(device)  # single image case\n","            else:\n","                images = [x.to(device) for x in images]  # dual images case\n","\n","            with torch.no_grad():\n","                outputs = model(images)\n","                preds = torch.argmax(outputs, 1)\n","\n","            if not isinstance(images, list):\n","                # single image case\n","                all_preds.extend(preds.cpu().numpy())\n","                image_ids = [\n","                    os.path.basename(test_loader.dataset.data[idx]['img_path']) for idx in\n","                    range(i * test_loader.batch_size, i * test_loader.batch_size + len(images))\n","                ]\n","                all_image_ids.extend(image_ids)\n","                if not test_only:\n","                    all_labels.extend(labels.numpy())\n","            else:\n","                # dual images case\n","                for k in range(2):\n","                    all_preds.extend(preds.cpu().numpy())\n","                    image_ids = [\n","                        os.path.basename(test_loader.dataset.data[idx][f'img_path{k + 1}']) for idx in\n","                        range(i * test_loader.batch_size, i * test_loader.batch_size + len(images[k]))\n","                    ]\n","                    all_image_ids.extend(image_ids)\n","                    if not test_only:\n","                        all_labels.extend(labels.numpy())\n","\n","            pbar.update(1)\n","\n","    # Save predictions to csv file for Kaggle online evaluation\n","    if test_only:\n","        df = pd.DataFrame({\n","            'ID': all_image_ids,\n","            'TARGET': all_preds\n","        })\n","        df.to_csv(prediction_path, index=False)\n","        print(f'[Test] Save predictions to {os.path.abspath(prediction_path)}')\n","    else:\n","        metrics = compute_metrics(all_preds, all_labels)\n","        return metrics\n","\n","\n","def compute_metrics(preds, labels, per_class=False):\n","    kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n","    accuracy = accuracy_score(labels, preds)\n","    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n","    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n","\n","    # Calculate and print precision and recall for each class\n","    if per_class:\n","        precision_per_class = precision_score(labels, preds, average=None, zero_division=0)\n","        recall_per_class = recall_score(labels, preds, average=None, zero_division=0)\n","        return kappa, accuracy, precision, recall, precision_per_class, recall_per_class\n","\n","    return kappa, accuracy, precision, recall\n","\n","\n","class MyModel(nn.Module):\n","    def __init__(self, num_classes=5, dropout_rate=0.5):\n","        super().__init__()\n","\n","        # Use pretrained=False since we're loading custom weights\n","\n","        #self.backbone = models.resnet18(pretrained=False)\n","        self.backbone = models.efficientnet_b0(pretrained=False)\n","        state_dict = torch.load('./pretrained/efficientnet_b0.pth')\n","        info = self.backbone.load_state_dict(state_dict, strict=False)\n","        print('missing keys:', info.missing_keys)  # Normal: FC layer will be missing\n","        print('unexpected keys:', info.unexpected_keys)\n","\n","        # Replace the classifier layer\n","        self.backbone.fc = nn.Sequential(\n","            nn.Linear(512, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=dropout_rate),\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=dropout_rate),\n","            nn.Linear(128, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        return self.backbone(x)\n","\n","\n","class MyDualModel(nn.Module):\n","    def __init__(self, num_classes=5, dropout_rate=0.5):\n","        super().__init__()\n","\n","        backbone = models.efficientnet_b0(pretrained=True)\n","        backbone.fc = nn.Identity()\n","\n","        # Here the two backbones will have the same structure but unshared weights\n","        self.backbone1 = copy.deepcopy(backbone)\n","        self.backbone2 = copy.deepcopy(backbone)\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(512 * 2, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=dropout_rate),\n","            nn.Linear(256, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=dropout_rate),\n","            nn.Linear(128, num_classes)\n","        )\n","\n","    def forward(self, images):\n","        image1, image2 = images\n","\n","        x1 = self.backbone1(image1)\n","        x2 = self.backbone2(image2)\n","\n","        x = torch.cat((x1, x2), dim=1)\n","        x = self.fc(x)\n","        return x\n","\n","\n","if __name__ == '__main__':\n","\n","    # Define the mode (single image pipeline)\n","    mode = 'single'  # forward single image to the model each time\n","    assert mode in ('single', 'dual')\n","\n","    # Create datasets\n","    train_dataset = RetinopathyDataset('./DeepDRiD/train.csv', './DeepDRiD/train/', transform_train, mode)\n","    val_dataset = RetinopathyDataset('./DeepDRiD/val.csv', './DeepDRiD/val/', transform_test, mode)\n","    test_dataset = RetinopathyDataset('./DeepDRiD/test.csv', './DeepDRiD/test/', transform_test, mode, test=True)\n","\n","    # Create dataloaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","    # Load pretrained backbone with Task (b)\n","    print(\"Using pretrained efficientnet_b0 backbone for Task (b)...\")\n","    model = MyModel(num_classes=5, dropout_rate=0.5)\n","    pretrained_weights_path = './pretrained/efficientnet_b0.pth'  # Path to pretrained weights\n","    state_dict = torch.load(pretrained_weights_path)  # Load weights\n","    info = model.backbone.load_state_dict(state_dict, strict=False)\n","    print(f\"Missing keys: {info.missing_keys}, Unexpected keys: {info.unexpected_keys}\")\n","\n","    # Move model to device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    # Fine-tune on DeepDRiD\n","    # Define loss, optimizer, and scheduler\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Unfreeze all layers for fine-tuning\n","    for param in model.parameters():\n","        param.requires_grad = True\n","\n","    # Optimizer and Learning rate scheduler\n","    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","    # Fine-tune the model\n","    model = train_model(\n","        model=model,\n","        train_loader=train_loader,\n","        val_loader=val_loader,\n","        device=device,\n","        criterion=criterion,\n","        optimizer=optimizer,\n","        lr_scheduler=lr_scheduler,\n","        num_epochs=10,  # Adjust as needed\n","        checkpoint_path='./model_task_b_finetuned_efficientnet_b0.pth'  # Save checkpoint for Task (b)\n","    )\n","\n","    # Save Predictions for Kaggle\n","    print(\"Saving test set predictions...\")\n","    state_dict = torch.load('./model_task_b_finetuned_efficientnet_b0.pth')\n","    model.load_state_dict(state_dict)\n","\n","    evaluate_model(\n","        model=model,\n","        test_loader=test_loader,\n","        device=device,\n","        test_only=True,\n","        prediction_path='./sample_submission_task_b_efficientnet_b0.csv'\n","    )\n","\n","    print(\"Task (b) complete! Predictions saved to './sample_submission_task_b_efficientnet_b0.csv'.\")\n"]}]}